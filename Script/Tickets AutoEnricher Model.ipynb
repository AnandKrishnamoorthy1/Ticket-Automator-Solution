{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Extract key phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import warnings,pickle\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn, pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data and analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd=pd.read_csv(r'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd['comment_orig']=df_pd['comment']\n",
    "df_pd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case_grp=df_pd[['case_type','i_customer_id']].groupby(['case_type']).count().sort_values(['i_customer_id'],asc)\n",
    "df_case_grp.loc[\"Debit card Reissuance\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd=df_pd[df_pd.case_type.isin([\"Debit card Reissuance\",\"Transaction Related\",\"Online transaction status\",\"Debit Card Usage\"])]\n",
    "print(len(df_pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd=df_pd[df_pd['comment'].str.len()>5]\n",
    "print(len(df_pd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding abbreviated test in comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbreviate_text(inp_df):\n",
    "    inp_df.comment=inp_df.comment.str.replace(r'a/c',\"account\")\n",
    "    inp_df.comment=inp_df.comment.str.replace(r'\\b(ac)\\b',\"account\")\n",
    "    inp_df.comment=inp_df.comment.str.replace(r'\\b(acc)\\b',\"account\")\n",
    "    inp_df.comment=inp_df.comment.str.replace(r'\\b(cm)\\b',\"customer\")\n",
    "    inp_df.comment=inp_df.comment.str.replace(r'\\b(cus)\\b',\"customer\")\n",
    "    inp_df.comment=inp_df.comment.str.replace(r'\\b(cust)\\b',\"customer\")\n",
    "    inp_df.comment=inp_df.comment.str.replace(r'\\b(d/c)\\b',\"debit card\")\n",
    "    inp_df.comment=inp_df.comment.str.replace(r'\\b(dc)\\b',\"debit card\")\n",
    "    inp_df.comment=inp_df.comment.str.replace(r'\\b(txn)\\b',\"transaction\")\n",
    "    inp_df.comment=inp_df.comment.str.replace(r'\\b(trxn)\\b',\"transaction\")\n",
    "    inp_df.comment=inp_df.comment.str.replace(r'\\b(amt)\\b',\"amount\")\n",
    "    inp_df.comment=inp_df.comment.str.replace(r'\\b(amnt)\\b',\"amount\")\n",
    "    inp_df.comment=inp_df.comment.str.replace(r'\\b(plz)\\b',\"please\")\n",
    "    inp_df.comment=inp_df.comment.str.replace(r'\\b(fd)\\b',\"fixed deposit\")\n",
    "    inp_df.comment=inp_df.comment.str.replace(r'\\b(rnew)\\b',\"renew\")\n",
    "    inp_df.comment=inp_df.comment.str.replace(r'\\b(sup)\\b',\"supervisor\")\n",
    "    inp_df.comment=inp_df.comment.str.replace(r'\\b(atd)\\b',\"atm withdrawal\")\n",
    "    return inp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert comment to lower case and call abbreviate fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd['comment']=df_pd.comment.str.lower()\n",
    "df_pd=abbreviate_text(df_pd)\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "df_pd['comment'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd['comment']=df_pd.comment.str.replace('imps/na/xxx/rrn:/pa/',\" \")\n",
    "df_pd['comment']=df_pd.comment.str.replace('\\W',\" \")\n",
    "df_pd['comment']=df_pd.comment.str.replace('[\\s+]',\"\",regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_stop=['please','kindly','check','dear','still','regards','customer']\n",
    "stop.extend(domain_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stop words from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(inp_df):\n",
    "    clean_data_lst=[]\n",
    "    for i,row in inp_df.iterrows():\n",
    "        tokens=str(row['comment']).split()\n",
    "        tokens=[i for i in tokens if not i in stop]\n",
    "        clean_data_lst.append(' '.join(word for word in tokens))\n",
    "    return clean_data_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove text with fewer inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd=df_pd[df_pd['comment'].str.len()>5]\n",
    "print(len(df_pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',-1)\n",
    "df_pd['comment'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining stopwords for RAKE algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_RAKE=stop[:]\n",
    "stop_RAKE.remove(\"didn't\")\n",
    "stop_RAKE.remove(\"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RAKE\n",
    "Rake=RAKE.Rake(stop_RAKE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting noun phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_phrase(inp_comment):\n",
    "    NP=\"NP: {(<V\\w+>|<NN\\w?>+.*<NN\\w?>)}\"\n",
    "    chnkr=nltk.RegexpParser(NP)\n",
    "    current_chunk=[]\n",
    "    cont_chunk=[]\n",
    "    tokens=word_tokenize(str(inp_comment))\n",
    "    tokens=[i for i in tokens if not i in stop]\n",
    "    text_chunk=chnkr.parse(nltk.pos_tag(tokens))\n",
    "    \n",
    "    for subtree in text_chunk:\n",
    "        if type(subtree)==nltk.Tree:\n",
    "            current_chunk.append(' '.join([word for word, POS in subtree.leaves()]))\n",
    "        elif current_chunk:\n",
    "            ne= ' '.join(current_chunk)\n",
    "            if ne not in cont_chunk:\n",
    "                cont_chunk.append(ne)\n",
    "                cont_chunk=[]\n",
    "    return ', '.join(cont_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicate key words from the noun phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_dup_ele(inp_lst,inp_comment):\n",
    "    result=[]\n",
    "    for string in inp_lst:\n",
    "        if not any([string in r for r in inp_lst if string!=r])\n",
    "            result.append(string)\n",
    "            \n",
    "    if len(result)<=6:\n",
    "        return result\n",
    "    else:\n",
    "        new_result=result[0:5]\n",
    "        result=result[5:]\n",
    "        noun_phrase=get_noun_phrase(inp_comment)\n",
    "        for phrase in result:\n",
    "            if any(phrase in np for np in noun_phrase):\n",
    "                new_result.append(phrase)\n",
    "        return new_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Key Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_key(inp_df):\n",
    "    inp_df['key_phrase']=''\n",
    "    for i,row in inp_df.iterrows():\n",
    "        cand_key=Rake.run(str(row.comment))\n",
    "        keys=', '.join(key[0] for key in cand_key)\n",
    "        keys_lst=keys.split(\",\")\n",
    "        if len(key_lst)>8:\n",
    "            inp_df.loc[i,'key_phrase']=', '.join(rem_dup_ele(keys_lst,str(row.comment)))\n",
    "        else:\n",
    "            inp_df.loc[i,'key_phrase']=keys\n",
    "        return inp_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_df=get_candidate_key(inp_df)\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "inp_df.iloc[:,-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2: Ticket case classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(inp_df):\n",
    "    clean_data_lst=[]\n",
    "    for i,row in inp_df.iterrows():\n",
    "        tokens=str(row['comment']).split()\n",
    "        tokens=[i for i in tokens if not i in stop]\n",
    "        clean_data_lst.append(' '.join(word for word in tokens))\n",
    "    return clean_data_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd['comment_token']=clean_data(df_pd)\n",
    "df_pd['comment_token'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping and analysing Ticket case types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grp=df_pd[['case_type','i_customer_id']].groupby(['case_type']).count().sort_values(['i_customer_id'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anand\\AppData\\Local\\Continuum\\anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "X=df_pd['comment']\n",
    "y=df_pd['case_type']\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)\n",
    "len(df_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizing the input data for ml model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect=TfidfVectorizer(max_features=1000,ngram_range=(1,3)).fit(X_train)\n",
    "train_vect=vect.transform(X_train)\n",
    "test_vect=vect.transform(X_test)\n",
    "with open('RF_Vect.pkl','wb')\n",
    "    pickle.dump(vect,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "np.array(df_pd.case_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(n_estimators=50,min_samples_leaf=25)\n",
    "clf.fit(train_vect,y_train)\n",
    "predict=clf.predict(test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RF_Model_new.pkl','wb')\n",
    "    pickle.dump(clf,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Train Score: \",clf.score(train_vect,y_train).round(2))\n",
    "print(\"Model Test Score: \",clf.score(test_vect,y_test).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred=pd.DataFfame(X_test).reset_index(drop=True)\n",
    "X_pred['Predicted Case']=pd.Series(predict)\n",
    "X_pred['actual Case']=y_test.rest_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category=np.unique(y_train)\n",
    "category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing confusion matrix for Ticket case types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,predict,labels=np.unique(y_train))\n",
    "print(cm)\n",
    "i=0\n",
    "for row in cm:\n",
    "    print(category[i]+\" : \"+str(df_case_grp.loc[category[i]].values[0])+\" : \"+str(row[i]*100/sum(row)))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd_relevent=df_pd[df_pd.case_type.isin([\"case related\",\"Debit Card Usage\",\"Online transmission Status\",\"Debit Card Reissued\"])]\n",
    "len(df_pd_relevent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vect=vect.transform(df_pd_relevent['comment'])\n",
    "df_pd_relevent=df_pd_relevent{\"case_type\"}\n",
    "print(\"Model Test Relevent Score: \",clf.score(X_test_vect_relevent,Y_test_relevent).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred[X_pred[\"actual case\"]==\"Supervisor Call back\"].iloc[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred[X_pred[\"actual case\"]==\"Foreclosure of Loan\"].iloc[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred.iloc[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Prioratize Tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the pretraining sentiment analyzer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vadar import SentimentIntensityAnalyzer\n",
    "sent=SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score priority based on ticket sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polarity_score(text):\n",
    "    ss=sent.polarity_score(text)\n",
    "    score=0\n",
    "    if ss['compound']>0.5:\n",
    "        score=1\n",
    "    elif ss['compound']>0.0:\n",
    "        score=2\n",
    "    elif ss['compound']>-0.4:\n",
    "        score=3\n",
    "    elif ss['compound']>-0.7:\n",
    "        score=4\n",
    "    else:\n",
    "        score=5\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_priority_score(inp_df):\n",
    "    inp_df['priority_score']=''\n",
    "    for i,row in inp_df.iterrows():\n",
    "        inp_df.loc[i,'priority_score']=get_priority_keywords(row)+get_priority_score(str(row.comment))\n",
    "    return inp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prioratize tickets and view the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_df=get_priority_score(df_pd.head(50))\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "inp_df.iloc[:,-3:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
